---
title: Keynotes
layout: default
---


<h1>AISTATS 2015 Keynote Presentations</h1>

<table border="0">

<tr><td colspan=2>
<DIV ID="session1"></DIV>
<h2>May 9 (Saturday) 17:00 - 18:00</h2>
</td></tr>

<tr>
<td valign="top"><img width="120" src="{{ site.baseurl }}/image/Baldi.jpg"></td>
<td valign="top">
<a onclick="window.open(this.href); return false;" href="http://www.igb.uci.edu/~pfbaldi/">Pierre Baldi</a>, 
<i>University of California, Irvine</i>

<p><b>The Ebb and Flow of Deep Learning: a Theory of Local Learning</b>

<p>
<b>Abstract</b> 
In a physical neural system, where storage and processing are intertwined, the learning rules for adjusting synaptic weights can only depend on local variables, such as the activity of the pre- and post-synaptic neurons. Thus learning models must specify
two things: (1) which variables are to be considered local; and (2) which kind of function combines these local variables into a learning rule. We consider polynomial learning rules and analyze their behavior
and capabilities in both linear and non-linear networks. As a byproduct, this framework enables the discovery of new learning rules and important relationships between learning rules and group symmetries.

<p>
Stacking local learning rules in deep feedforward networks leads to deep local learning. While deep local learning can learn interesting representations, it cannot learn complex input-output functions, even when targets are available for the top layer. Learning complex input-output functions
requires instead local deep learning, where target information is transmitted to the deep layers, thereby raising two fundamental issues: (1) the nature of the transmission channel; and (2)
the nature and amount of information transmitted over this channel. This leads to the class of deep targets learning algorithms, which provide targets for the deep layers, and its stratification along the information spectrum,
illuminating the remarkable power and uniqueness of the backpropation algorithm. The theory clarifies the concept of Hebbian learning, what is learnable by Hebbian learning, and explains the sparsity of the space of learning rules discovered so far and the unique role
backpropagation plays in this space.

<p>
<b>Bio</b> 
Pierre Baldi earned MS degrees in Mathematics and Psychology from the University of Paris, and a PhD in Mathematics from the California Institute of Technology. He is currently Chancellor's Professor in the Department of Computer Science,
Director of the Institute for Genomics and Bioinformatics, and Associate Director of the Center for Machine Learning and Intelligent Systems at the University of California Irvine. The long term focus of his research is on understanding
intelligence in brains and machines. He has pioneered the development and application of deep learning methods to problems in the natural sciences such as the detection of exotic particles in physics,
the prediction of reactions in chemistry, and the prediction of protein secondary and tertiary structure in biology. He is the recipient of the 1993 Lew Allen Award at JPL, the 1999 Laurel Wilkening Faculty Innovation Award at UCI, a 2006 Microsoft Research Award, the 2010 E. R. Caianiello Prize for research in
machine learning, and a 2014 Google Faculty Research Award. He is and Elected Fellow of the AAAS, AAAI, IEEE, ACM, and ISCB.

</td>
</tr>


<tr><td colspan=2>
<DIV ID="session2"></DIV>
<h2>May 10 (Sunday) 08:30 - 09:30</h2>
</td></tr>

<tr>
<td valign="top"><img width="120" src="{{ site.baseurl }}/image/Yu.jpg"></td>
<td valign="top">
<a onclick="window.open(this.href); return false;" href="http://www.dbs.ifi.lmu.de/~yu_k/">Kai Yu</a>, 
<i>Baidu</i>

<p><b>How Deep Learning Transforms Chinese Internet Search</b>

<p>
<b>Abstract</b> 
Since 2012, the Chinese internet giant Baidu has launched a significant effort to develop deep learning technologies to transform its search engine to the next stage. The company established  Institute of Deep Learning (IDL), the first research team of Baidu and later expanded the research force to a bigger organization Baidu Research. By leveraging big data and massive parallel computation on CPU and GPU servers, Baidu researchers have achieved transformative improvements to Baidu's core business, such as web search, online advertising, speech recognition, computer vision, natural language processing, smart data center, robotics, etc.. In this talk I will showcase some of the highlights, and introduce the underlying technologies. These technologies have been shaping up a foundation for imaginative long-term innovation of Baidu. In the end I will discuss some thoughts about the future directions of deep learning.

<p>
<b>Bio</b>
Dr. Yu Kai, founder and Chief Executive Officer of Horizon Robotics, is a world-renowned machine learning expert, and an important promoter of the application of deep-learning technology in China, especially in the smart vehicle sector.

In 2015, Dr. Yu Kai founded Horizon Robotics, a leading supplier of energy-efficient computing solutions for smart vehicles. Under his leadership, Horizon Robotics first realized the commercialization of driver assistance technology in China's passenger-vehicle sector through the integration of hardware and software. Since 2019, Horizon Robotics launched a series of automotive processors, Journey 2, Journey 3 and Journey 5. In 2020, Horizon Robotics kicked off the first year of mass production of preinstallation of China's automotive-grade processors from 0 to 1. To date, Horizon Robotics has signed mass production projects for more than 70 vehicle models with more than 20 vehicle companies while the cumulative shipments of its processors have exceeded 2 million as the company collaborates with partners to explore and create value for the sector from 1 to N. 

From 2012 to 2015, Dr. Yu Kai was executive head of Baidu Institute of Deep Learning (IDL) and executive president of Baidu Research. He established and led projects including IDL, Baidu's automated driving team and Baidu Deep Learning Platform PaddlePaddle, and led his team to win the "Baidu highest award" three times. From 2006 to 2012, Dr. Yu Kai served as director of the media laboratory at NEC Labs America (one of the pioneering laboratories in the world engaged in the research and development of convolutional neural networks). In 2011, Dr. Yu was invited as an Adjunct Faculty to teach a graduate-level class in the Computer Science Department of Stanford University. In his academic research career, Dr. Yu has published over 100 research papers in prestigious conferences and journels, being cited by peers for over 30,000 times.  

Yu Kai received his Bachelor's Degree and Master's Degree from Nanjing University in 1998 and 2000 respectively, and obtained his PhD degree in Computer Science from University of Munich in Germany in 2004.


</td>
</tr>


<tr><td colspan=2>
<DIV ID="session3"></DIV>
<h2>May 11 (Monday) 08:30 - 09:30</h2>
</td></tr>

<tr>
<td valign="top"><img width="120" src="{{ site.baseurl }}/image/Hansen.jpg"></td>
<td valign="top">
<a onclick="window.open(this.href); return false;" href="http://www.journalism.columbia.edu/profile/428-mark-hansen/10">Mark Hansen</a>, 
<i>Columbia University</i>

<p><b>Data and/as Journalism</b>

<p>
<b>Abstract</b> 
As statisticians know well, data and data technologies have profound 
social and political relevance -- almost every aspect of our lives can 
"rendered" in data. These data tell us stories about who we are and 
how we live, but, as products of human attention, innovation and 
memory, their perspective is not neutral and the stories they tell are 
often incomplete, open-ended and biased.

Data can either reinforce or challenge systems of power and so, in a 
very real sense, our democracy depends on the public being able to 
think critically about these technologies, telling the good stories 
from the bad. As "truth-tellers, sense-makers and explainers of last 
resort," to quote my colleague Emily Bell, journalists needs to 
understand the working of data in society, and the ways in which data, 
code and algorithms function.

In this talk, I will look at how journalism is adapting to a world of 
"Big Data" and "Data Science." From a pedagogical perspective, 
journalism acts as a kind of "integrator" of real world problems with 
more technical statistical and computational perspectives. The 
practice of journalism fills a gap between lived experience and data, 
its analysis and its visualization -- and might	even expand the way we 
teach statistics.

<p>
<b>Bio</b> 
Mark Hansen is a Professor in the Columbia Graduate School of 
Journalism, where he heads the Brown Institute for Media 
Innovation. Prior to that he was a Professor in the Department of 
Statistics at UCLA and a Member of the Technical Staff at Bell 
Laboratories. He is also co-founder of the Office for Creative 
Research, a design group that has exhibited work at the Museum of 
Modern Art in New York, the Whitney Museum, the Centro de Arte Reina 
Sofia, the London Science Museum, the Cartier Foundation in Paris, and 
the lobbies of the New York Times building and the Public Theater 
(permanent displays) in Manhattan. He received a B.S. in Applied Math 
from UC Davis and a Ph.D. in Statistics from UC Berkeley.


</td>
</tr>


<tr><td colspan=2>
<DIV ID="session4"></DIV>
<h2>May 12 (Tuesday) 08:30 - 09:30</h2>
</td></tr>

<tr>
<td valign="top"><img width="120" src="{{ site.baseurl }}/image/Agarwal.jpg"></td>
<td valign="top">
<a onclick="window.open(this.href); return false;" href="https://www.linkedin.com/in/dipu1025">Deepak Agarwal</a>, 
<i>LinkedIn</i>

<p><b>Scaling Machine Learning and Statistics for Web Applications</b>

<p>
<b>Abstract</b> 
Scaling web applications like recommendation systems, search and computational advertising is challenging. Such systems have to make astronomical number of decisions every day on what to serve users when they are using the website and/or the mobile app. Machine learning and statistical modeling approaches that can obtain insights by continuously processing large amounts of data emitted at very high frequency by these applications have emerged as the method of choice. However, there are three challenges to scale such methods :  a) scientific  b)infrastructure and c) organizational.  I will provide an overview of these challenges and the strategies we have adopted at LinkedIn to address those. Throughout, I will illustrate with examples from real-world applications at LinkedIn.

<p>
<b>Bio</b>
Big data analyst with 15 years of experience developing and deploying state-of-the-art machine learning and statistical methods for improving the relevance of web applications. Have worked in various positions: chief scientist of large projects, managed small and highly technical teams and also experienced in managing large teams (almost all PhDs). Most recently, leading a team of 60+ scientists and developers at LinkedIn. Also experienced in conducting new scientific research to solve notoriously difficult big data problems, especially in the areas of recommender systems and computational advertising. Fellow of the American Statistical Association, Member Board of Directors for SIGKDD, program chair of KDD in the past, associate editor of two top-tier journals in Statistics, regularly serves on senior program committees of top-tier conferences like KDD, NIPS, CIKM, ICDM, SIGIR, WSDM. 


</td>
</tr>





</table>




